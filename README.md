# TIAKT: Tri-level Interactive Attention Knowledge Tracing

##âš ï¸ Note
This repository contains pseudocode that illustrates the core mechanisms of our TIAKT model. The pseudocode is designed to help readers understand the key algorithmic ideas and model architecture presented in our paper. It is not intended to be directly executable but rather serves as a reference for understanding the methodology. If you have any questions or require further clarification, please feel free to contact us.
w18724284923@outlook.com
## ğŸ“‹ Overview

TIAKT is a novel knowledge tracing model that incorporates three levels of memory interaction inspired by educational psychology:

1. **Short-term Memory Encoder**: Captures recent learning patterns using monotonic attention
2. **Advance Organizer Module**: Activates relevant prior knowledge based on current task
3. **Neural Memory Module**: Dynamically updates knowledge state with gating mechanism
4. **Persistent Memory Module**: Consolidates long-term stable knowledge
5. **Memory Fusion Module**: Integrates multi-level memories for prediction

## ğŸ—ï¸ Architecture

```
Input: (questions, answers, problem_ids)
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Multi-level Embedding Layer     â”‚
â”‚   e_t^q = e_base + Î¼_t * e_diff     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Short-term Memory Encoder        â”‚
â”‚  (Dual-path Transformer with        â”‚
â”‚   Monotonic Attention)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Advance Organizer Module        â”‚
â”‚  Î±_t = softmax(hÂ·W_aÂ·e_t^q)         â”‚
â”‚  a_t = Î£ Î±_t,i Â· h_i                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Neural Memory Module            â”‚
â”‚  Î²_t = Ïƒ(w_Î²Â·s_t + b_Î²)             â”‚
â”‚  M^t = Î²Â·M^{t-1} + (1-Î²)Â·M^curr     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Persistent Memory Module         â”‚
â”‚  P^t = W_1Â·P^{t-1} + W_2Â·MÌ„ + b     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Memory Fusion (Transformer)     â”‚
â”‚  C = Fuse([S; M; P])                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Prediction Layer                â”‚
â”‚  p_t = Ïƒ(FC([C_t; e_t^q]))          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
Output: Probability of correct answer
```

## ğŸ“ Project Structure

```
TIAKT/
â”œâ”€â”€ pseudocode/
â”‚   â”œâ”€â”€ tiakt_pseudocode.py      # Core algorithm pseudocode
â”‚   â””â”€â”€ README.md                # This file
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ assist2009_pid/          # ASSISTments 2009 dataset
â”‚   â”œâ”€â”€ assist2017_pid/          # ASSISTments 2017 dataset
â”‚   â”œâ”€â”€ assist2015/              # ASSISTments 2015 dataset
â”‚   â””â”€â”€ statics/                 # Statics 2011 dataset
â””â”€â”€ results/                     # Experimental results
```

## ğŸ”¬ Key Components

### 1. Monotonic Attention (from AKT)
```python
# Position-aware exponential decay
position_distance = |i - j|
decay = exp(Î³ * position_distance)
attention = softmax(QK^T / âˆšd * decay)
```

### 2. Advance Organizer
```python
# Task-oriented biased attention
Î±_t = softmax(h_i^T Â· W_a Â· e_t^q / âˆšd)
prior_knowledge = Î£ Î±_t,i Â· h_i
s_t^short = Ïƒ(W_h Â· [e_t^q; prior_knowledge])
```

### 3. Gated Memory Update
```python
# Neural memory with gating
gate = Ïƒ(W_gate Â· s_t + b_gate)
M^t = gate âŠ™ M^{t-1} + (1 - gate) âŠ™ candidate
```

### 4. Multi-level Fusion
```python
# Transformer-based fusion
Z_in = concat([S^short, M^t, P^t])
Z_out = TransformerEncoder(Z_in)
C = Z_out[:seq_len]  # Cognitive state
```

### Cross-Task Transfer

- Zero-shot transfer between datasets
- Fine-tuning with frozen Transformer layers
- Analysis of transferable components

### Supplementary Experiments

- Cold start performance (low-frequency skills)
- Sequence length analysis
- Position effect analysis
- Difficulty-based analysis
- 
### Hardware Environment
GPU: NVIDIA GPU with CUDA support
Framework: PyTorch
Python: 3.8+

## ğŸ“ License

This code is released for academic research purposes only.

## ğŸ™ Acknowledgments

This work builds upon:
- AKT (Ghosh et al., 2020) for monotonic attention mechanism
- Educational psychology theories (Ausubel's Advance Organizer)
